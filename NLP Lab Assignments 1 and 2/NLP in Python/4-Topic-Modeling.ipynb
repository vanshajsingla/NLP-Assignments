{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                    ASSIGNMENT -5 (TOPIC MODELING)\n",
    "                    NAME: VANSHAJ SINGLA               ROLL NO: 102003346                         GROUP: CO14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>...</th>\n",
       "      <th>zee</th>\n",
       "      <th>zen</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>Ã©clair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 7468 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaah  aaaaahhhhhhh  aaaaauuugghhhhhh  aaaahhhhh  aaah  aah  abc  \\\n",
       "ali           0             0                 0          0     0    0    1   \n",
       "anthony       0             0                 0          0     0    0    0   \n",
       "bill          1             0                 0          0     0    0    0   \n",
       "bo            0             1                 1          1     0    0    0   \n",
       "dave          0             0                 0          0     1    0    0   \n",
       "hasan         0             0                 0          0     0    0    0   \n",
       "jim           0             0                 0          0     0    0    0   \n",
       "joe           0             0                 0          0     0    0    0   \n",
       "john          0             0                 0          0     0    0    0   \n",
       "louis         0             0                 0          0     0    3    0   \n",
       "mike          0             0                 0          0     0    0    0   \n",
       "ricky         0             0                 0          0     0    0    0   \n",
       "\n",
       "         abcs  ability  abject  ...  zee  zen  zeppelin  zero  zillion  \\\n",
       "ali         0        0       0  ...    0    0         0     0        0   \n",
       "anthony     0        0       0  ...    0    0         0     0        0   \n",
       "bill        1        0       0  ...    0    0         0     1        1   \n",
       "bo          0        1       0  ...    0    0         0     1        0   \n",
       "dave        0        0       0  ...    0    0         0     0        0   \n",
       "hasan       0        0       0  ...    2    1         0     1        0   \n",
       "jim         0        0       0  ...    0    0         0     0        0   \n",
       "joe         0        0       0  ...    0    0         0     0        0   \n",
       "john        0        0       0  ...    0    0         0     0        0   \n",
       "louis       0        0       0  ...    0    0         0     2        0   \n",
       "mike        0        0       0  ...    0    0         2     1        0   \n",
       "ricky       0        1       1  ...    0    0         0     0        0   \n",
       "\n",
       "         zombie  zombies  zoning  zoo  Ã©clair  \n",
       "ali           1        0       0    0       0  \n",
       "anthony       0        0       0    0       0  \n",
       "bill          1        1       1    0       0  \n",
       "bo            0        0       0    0       0  \n",
       "dave          0        0       0    0       0  \n",
       "hasan         0        0       0    0       0  \n",
       "jim           0        0       0    0       0  \n",
       "joe           0        0       0    0       0  \n",
       "john          0        0       0    0       1  \n",
       "louis         0        0       0    0       0  \n",
       "mike          0        0       0    0       0  \n",
       "ricky         0        0       0    1       0  \n",
       "\n",
       "[12 rows x 7468 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ali</th>\n",
       "      <th>anthony</th>\n",
       "      <th>bill</th>\n",
       "      <th>bo</th>\n",
       "      <th>dave</th>\n",
       "      <th>hasan</th>\n",
       "      <th>jim</th>\n",
       "      <th>joe</th>\n",
       "      <th>john</th>\n",
       "      <th>louis</th>\n",
       "      <th>mike</th>\n",
       "      <th>ricky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaaaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ali  anthony  bill  bo  dave  hasan  jim  joe  john  louis  \\\n",
       "aaaaah              0        0     1   0     0      0    0    0     0      0   \n",
       "aaaaahhhhhhh        0        0     0   1     0      0    0    0     0      0   \n",
       "aaaaauuugghhhhhh    0        0     0   1     0      0    0    0     0      0   \n",
       "aaaahhhhh           0        0     0   1     0      0    0    0     0      0   \n",
       "aaah                0        0     0   0     1      0    0    0     0      0   \n",
       "\n",
       "                  mike  ricky  \n",
       "aaaaah               0      0  \n",
       "aaaaahhhhhhh         0      0  \n",
       "aaaaauuugghhhhhh     0      0  \n",
       "aaaahhhhh            0      0  \n",
       "aaah                 0      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.matutils.Sparse2Corpus object at 0x00000265E1AFF2B0>\n",
      "CountVectorizer(stop_words=frozenset({'a', 'about', 'above', 'across', 'after',\n",
      "                                      'afterwards', 'again', 'against', 'all',\n",
      "                                      'almost', 'alone', 'along', 'already',\n",
      "                                      'also', 'although', 'always', 'am',\n",
      "                                      'among', 'amongst', 'amoungst', 'amount',\n",
      "                                      'an', 'and', 'another', 'any', 'anyhow',\n",
      "                                      'anyone', 'anything', 'anyway',\n",
      "                                      'anywhere', ...}))\n"
     ]
    }
   ],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n",
    "print(corpus)\n",
    "print(cv)\n",
    "# print(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"fucking\" + 0.006*\"say\" + 0.006*\"fuck\" + 0.006*\"shit\" + 0.005*\"going\" + 0.005*\"theyre\" + 0.005*\"didnt\" + 0.005*\"want\" + 0.005*\"hes\" + 0.005*\"thing\"'),\n",
       " (1,\n",
       "  '0.005*\"shit\" + 0.004*\"ok\" + 0.004*\"lot\" + 0.003*\"wanna\" + 0.003*\"gotta\" + 0.003*\"husband\" + 0.003*\"cause\" + 0.003*\"day\" + 0.003*\"women\" + 0.002*\"pregnant\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"fucking\" + 0.006*\"theyre\" + 0.006*\"say\" + 0.006*\"fuck\" + 0.006*\"theres\" + 0.005*\"cause\" + 0.005*\"hes\" + 0.005*\"really\" + 0.005*\"life\" + 0.005*\"thing\"'),\n",
       " (1,\n",
       "  '0.014*\"fucking\" + 0.008*\"shit\" + 0.007*\"fuck\" + 0.007*\"went\" + 0.006*\"going\" + 0.006*\"want\" + 0.006*\"thing\" + 0.006*\"day\" + 0.005*\"theyre\" + 0.005*\"hes\"'),\n",
       " (2,\n",
       "  '0.006*\"shit\" + 0.005*\"say\" + 0.005*\"dad\" + 0.005*\"didnt\" + 0.004*\"did\" + 0.004*\"going\" + 0.004*\"day\" + 0.004*\"good\" + 0.004*\"want\" + 0.004*\"guys\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"fucking\" + 0.009*\"shit\" + 0.008*\"fuck\" + 0.007*\"theyre\" + 0.006*\"didnt\" + 0.006*\"hes\" + 0.006*\"theres\" + 0.006*\"say\" + 0.005*\"thing\" + 0.005*\"man\"'),\n",
       " (1,\n",
       "  '0.005*\"little\" + 0.005*\"love\" + 0.004*\"say\" + 0.004*\"bo\" + 0.004*\"old\" + 0.004*\"stuff\" + 0.004*\"clinton\" + 0.004*\"way\" + 0.004*\"repeat\" + 0.004*\"hey\"'),\n",
       " (2,\n",
       "  '0.006*\"going\" + 0.005*\"want\" + 0.005*\"dad\" + 0.005*\"says\" + 0.005*\"say\" + 0.004*\"really\" + 0.004*\"cause\" + 0.004*\"love\" + 0.004*\"shit\" + 0.004*\"shes\"'),\n",
       " (3,\n",
       "  '0.010*\"fucking\" + 0.009*\"went\" + 0.008*\"day\" + 0.006*\"thing\" + 0.006*\"going\" + 0.006*\"say\" + 0.006*\"fuck\" + 0.005*\"good\" + 0.005*\"ive\" + 0.005*\"goes\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #2 (Nouns Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] =='NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies and gentlemen please welcome to the sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank you thank you thank you san francisco th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>all right thank you thank you very much thank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>bo what old macdonald had a farm e i e i o and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>this is dave he tells dirty jokes for a living...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>whats up davis whats up im home i had to bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies and gentlemen please welcome to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies and gentlemen welcome joe rogan  wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>all right petunia wish me luck out there you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>introfade the music out lets roll hold there l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thank you thanks thank you guys hey se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello hello how you doing great thank you wow ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies and gentlemen please welcome to the sta...\n",
       "anthony  thank you thank you thank you san francisco th...\n",
       "bill      all right thank you thank you very much thank...\n",
       "bo       bo what old macdonald had a farm e i e i o and...\n",
       "dave     this is dave he tells dirty jokes for a living...\n",
       "hasan      whats up davis whats up im home i had to bri...\n",
       "jim         ladies and gentlemen please welcome to the ...\n",
       "joe         ladies and gentlemen welcome joe rogan  wha...\n",
       "john     all right petunia wish me luck out there you w...\n",
       "louis    introfade the music out lets roll hold there l...\n",
       "mike     wow hey thank you thanks thank you guys hey se...\n",
       "ricky    hello hello how you doing great thank you wow ..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies gentlemen stage ali hi thank hello na s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank thank people i em i francisco city world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>thank thank pleasure georgia area oasis i june...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>macdonald farm e i o farm pig e i i snort macd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>jokes living stare work profound train thought...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>whats davis whats home i netflix la york i son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies gentlemen stage mr jim jefferies thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies gentlemen joe fuck thanks phone fuckfac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>petunia thats hello hello chicago thank crowd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>music lets lights lights thank i i place place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thanks look insane years everyone i id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello thank fuck thank im gon youre weve money...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies gentlemen stage ali hi thank hello na s...\n",
       "anthony  thank thank people i em i francisco city world...\n",
       "bill     thank thank pleasure georgia area oasis i june...\n",
       "bo       macdonald farm e i o farm pig e i i snort macd...\n",
       "dave     jokes living stare work profound train thought...\n",
       "hasan    whats davis whats home i netflix la york i son...\n",
       "jim      ladies gentlemen stage mr jim jefferies thank ...\n",
       "joe      ladies gentlemen joe fuck thanks phone fuckfac...\n",
       "john     petunia thats hello hello chicago thank crowd ...\n",
       "louis    music lets lights lights thank i i place place...\n",
       "mike     wow hey thanks look insane years everyone i id...\n",
       "ricky    hello thank fuck thank im gon youre weve money..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "# type(data_clean.transcript.to_string())\n",
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vansh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abortions</th>\n",
       "      <th>abuse</th>\n",
       "      <th>...</th>\n",
       "      <th>yummy</th>\n",
       "      <th>ze</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zee</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoo</th>\n",
       "      <th>Ã©clair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 4635 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaahhhhhhh  aaaaauuugghhhhhh  aaaahhhhh  aah  abc  abcs  ability  \\\n",
       "ali                 0                 0          0    0    1     0        0   \n",
       "anthony             0                 0          0    0    0     0        0   \n",
       "bill                0                 0          0    0    0     1        0   \n",
       "bo                  1                 1          1    0    0     0        1   \n",
       "dave                0                 0          0    0    0     0        0   \n",
       "hasan               0                 0          0    0    0     0        0   \n",
       "jim                 0                 0          0    0    0     0        0   \n",
       "joe                 0                 0          0    0    0     0        0   \n",
       "john                0                 0          0    0    0     0        0   \n",
       "louis               0                 0          0    3    0     0        0   \n",
       "mike                0                 0          0    0    0     0        0   \n",
       "ricky               0                 0          0    0    0     0        1   \n",
       "\n",
       "         abortion  abortions  abuse  ...  yummy  ze  zealand  zee  zeppelin  \\\n",
       "ali             0          0      0  ...      0   0        0    0         0   \n",
       "anthony         2          0      0  ...      0   0       10    0         0   \n",
       "bill            0          0      0  ...      0   1        0    0         0   \n",
       "bo              0          0      0  ...      0   0        0    0         0   \n",
       "dave            0          1      0  ...      0   0        0    0         0   \n",
       "hasan           0          0      0  ...      0   0        0    1         0   \n",
       "jim             0          0      0  ...      0   0        0    0         0   \n",
       "joe             0          0      1  ...      0   0        0    0         0   \n",
       "john            0          0      0  ...      0   0        0    0         0   \n",
       "louis           0          0      0  ...      0   0        0    0         0   \n",
       "mike            0          0      0  ...      0   0        0    0         2   \n",
       "ricky           0          0      0  ...      1   0        0    0         0   \n",
       "\n",
       "         zillion  zombie  zombies  zoo  Ã©clair  \n",
       "ali            0       1        0    0       0  \n",
       "anthony        0       0        0    0       0  \n",
       "bill           1       1        1    0       0  \n",
       "bo             0       0        0    0       0  \n",
       "dave           0       0        0    0       0  \n",
       "hasan          0       0        0    0       0  \n",
       "jim            0       0        0    0       0  \n",
       "joe            0       0        0    0       0  \n",
       "john           0       0        0    0       1  \n",
       "louis          0       0        0    0       0  \n",
       "mike           0       0        0    0       0  \n",
       "ricky          0       0        0    1       0  \n",
       "\n",
       "[12 rows x 4635 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"shit\" + 0.011*\"thing\" + 0.010*\"day\" + 0.008*\"man\" + 0.008*\"fuck\" + 0.008*\"life\" + 0.008*\"hes\" + 0.007*\"cause\" + 0.007*\"lot\" + 0.007*\"guy\"'),\n",
       " (1,\n",
       "  '0.007*\"dad\" + 0.007*\"way\" + 0.006*\"day\" + 0.005*\"life\" + 0.005*\"night\" + 0.005*\"shes\" + 0.005*\"hes\" + 0.005*\"mom\" + 0.005*\"parents\" + 0.005*\"man\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"shit\" + 0.009*\"fuck\" + 0.009*\"man\" + 0.008*\"cause\" + 0.007*\"way\" + 0.006*\"kind\" + 0.006*\"guy\" + 0.006*\"gon\" + 0.006*\"lot\" + 0.006*\"things\"'),\n",
       " (1,\n",
       "  '0.011*\"day\" + 0.010*\"thing\" + 0.009*\"life\" + 0.009*\"shit\" + 0.008*\"hes\" + 0.007*\"cause\" + 0.007*\"guy\" + 0.007*\"dad\" + 0.007*\"way\" + 0.006*\"man\"'),\n",
       " (2,\n",
       "  '0.007*\"joke\" + 0.007*\"hes\" + 0.007*\"stuff\" + 0.007*\"thing\" + 0.006*\"day\" + 0.006*\"bo\" + 0.006*\"man\" + 0.006*\"years\" + 0.005*\"id\" + 0.005*\"repeat\"')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"thing\" + 0.011*\"day\" + 0.008*\"life\" + 0.008*\"cause\" + 0.007*\"hes\" + 0.007*\"way\" + 0.007*\"things\" + 0.007*\"joke\" + 0.006*\"guy\" + 0.006*\"man\"'),\n",
       " (1,\n",
       "  '0.015*\"shit\" + 0.009*\"dude\" + 0.009*\"gon\" + 0.008*\"day\" + 0.008*\"hes\" + 0.007*\"thing\" + 0.007*\"lot\" + 0.007*\"guy\" + 0.006*\"man\" + 0.006*\"cause\"'),\n",
       " (2,\n",
       "  '0.013*\"dad\" + 0.009*\"mom\" + 0.008*\"shes\" + 0.007*\"life\" + 0.007*\"hey\" + 0.007*\"parents\" + 0.007*\"day\" + 0.007*\"way\" + 0.006*\"house\" + 0.006*\"clinton\"'),\n",
       " (3,\n",
       "  '0.014*\"shit\" + 0.014*\"fuck\" + 0.013*\"man\" + 0.007*\"lot\" + 0.006*\"way\" + 0.006*\"things\" + 0.006*\"guy\" + 0.006*\"house\" + 0.006*\"life\" + 0.006*\"gon\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies gentlemen welcome stage ali wong hi wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank san francisco thank good people surprise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>right thank thank pleasure greater atlanta geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>old macdonald farm e i i o farm pig e i i snor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>dirty jokes living stare most hard work profou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>whats davis whats im home i netflix special la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies gentlemen welcome stage mr jim jefferie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies gentlemen joe fuck san francisco thanks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>right petunia august thats good right hello he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>music lets lights lights thank much i i i nice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thanks hey seattle nice look crazy ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello great thank fuck thank lovely welcome im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies gentlemen welcome stage ali wong hi wel...\n",
       "anthony  thank san francisco thank good people surprise...\n",
       "bill     right thank thank pleasure greater atlanta geo...\n",
       "bo       old macdonald farm e i i o farm pig e i i snor...\n",
       "dave     dirty jokes living stare most hard work profou...\n",
       "hasan    whats davis whats im home i netflix special la...\n",
       "jim      ladies gentlemen welcome stage mr jim jefferie...\n",
       "joe      ladies gentlemen joe fuck san francisco thanks...\n",
       "john     right petunia august thats good right hello he...\n",
       "louis    music lets lights lights thank much i i i nice...\n",
       "mike     wow hey thanks hey seattle nice look crazy ins...\n",
       "ricky    hello great thank fuck thank lovely welcome im..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>ze</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zee</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoo</th>\n",
       "      <th>Ã©clair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 5587 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaah  aaaaahhhhhhh  aaaaauuugghhhhhh  aaaahhhhh  aah  abc  abcs  \\\n",
       "ali           0             0                 0          0    0    1     0   \n",
       "anthony       0             0                 0          0    0    0     0   \n",
       "bill          1             0                 0          0    0    0     1   \n",
       "bo            0             1                 1          1    0    0     0   \n",
       "dave          0             0                 0          0    0    0     0   \n",
       "hasan         0             0                 0          0    0    0     0   \n",
       "jim           0             0                 0          0    0    0     0   \n",
       "joe           0             0                 0          0    0    0     0   \n",
       "john          0             0                 0          0    0    0     0   \n",
       "louis         0             0                 0          0    3    0     0   \n",
       "mike          0             0                 0          0    0    0     0   \n",
       "ricky         0             0                 0          0    0    0     0   \n",
       "\n",
       "         ability  abject  able  ...  ze  zealand  zee  zeppelin  zero  \\\n",
       "ali            0       0     2  ...   0        0    0         0     0   \n",
       "anthony        0       0     0  ...   0       10    0         0     0   \n",
       "bill           0       0     1  ...   1        0    0         0     0   \n",
       "bo             1       0     0  ...   0        0    0         0     1   \n",
       "dave           0       0     0  ...   0        0    0         0     0   \n",
       "hasan          0       0     1  ...   0        0    2         0     0   \n",
       "jim            0       0     1  ...   0        0    0         0     0   \n",
       "joe            0       0     2  ...   0        0    0         0     0   \n",
       "john           0       0     3  ...   0        0    0         0     0   \n",
       "louis          0       0     1  ...   0        0    0         0     0   \n",
       "mike           0       0     0  ...   0        0    0         2     0   \n",
       "ricky          1       1     2  ...   0        0    0         0     0   \n",
       "\n",
       "         zillion  zombie  zombies  zoo  Ã©clair  \n",
       "ali            0       1        0    0       0  \n",
       "anthony        0       0        0    0       0  \n",
       "bill           1       1        1    0       0  \n",
       "bo             0       0        0    0       0  \n",
       "dave           0       0        0    0       0  \n",
       "hasan          0       0        0    0       0  \n",
       "jim            0       0        0    0       0  \n",
       "joe            0       0        0    0       0  \n",
       "john           0       0        0    0       1  \n",
       "louis          0       0        0    0       0  \n",
       "mike           0       0        0    0       0  \n",
       "ricky          0       0        0    1       0  \n",
       "\n",
       "[12 rows x 5587 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names_out())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"mom\" + 0.003*\"joke\" + 0.003*\"ass\" + 0.003*\"um\" + 0.003*\"son\" + 0.003*\"door\" + 0.003*\"comedy\" + 0.003*\"bo\" + 0.002*\"mad\" + 0.002*\"jokes\"'),\n",
       " (1,\n",
       "  '0.003*\"joke\" + 0.003*\"parents\" + 0.003*\"clinton\" + 0.003*\"jenny\" + 0.003*\"wife\" + 0.002*\"mom\" + 0.002*\"dog\" + 0.002*\"hell\" + 0.002*\"nuts\" + 0.002*\"dead\"')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"friend\" + 0.004*\"joke\" + 0.004*\"parents\" + 0.004*\"mom\" + 0.003*\"bo\" + 0.003*\"comedy\" + 0.003*\"jenny\" + 0.003*\"clinton\" + 0.003*\"repeat\" + 0.003*\"gay\"'),\n",
       " (1,\n",
       "  '0.005*\"mom\" + 0.005*\"hasan\" + 0.003*\"parents\" + 0.003*\"husband\" + 0.003*\"gun\" + 0.003*\"ok\" + 0.003*\"brown\" + 0.003*\"love\" + 0.003*\"bike\" + 0.002*\"door\"'),\n",
       " (2,\n",
       "  '0.005*\"joke\" + 0.004*\"ass\" + 0.004*\"guns\" + 0.003*\"jenner\" + 0.003*\"girlfriend\" + 0.003*\"cunt\" + 0.003*\"class\" + 0.003*\"jokes\" + 0.003*\"earth\" + 0.003*\"dick\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"joke\" + 0.004*\"jokes\" + 0.004*\"ahah\" + 0.004*\"mad\" + 0.003*\"anthony\" + 0.003*\"door\" + 0.003*\"dead\" + 0.003*\"jenner\" + 0.003*\"gay\" + 0.003*\"nigga\"'),\n",
       " (1,\n",
       "  '0.005*\"bo\" + 0.005*\"hasan\" + 0.004*\"repeat\" + 0.004*\"mom\" + 0.004*\"comedy\" + 0.004*\"parents\" + 0.004*\"contact\" + 0.004*\"love\" + 0.004*\"eye\" + 0.003*\"brown\"'),\n",
       " (2,\n",
       "  '0.005*\"jenny\" + 0.005*\"guns\" + 0.004*\"ass\" + 0.004*\"dog\" + 0.004*\"tit\" + 0.004*\"girls\" + 0.004*\"date\" + 0.004*\"class\" + 0.003*\"girlfriend\" + 0.003*\"morning\"'),\n",
       " (3,\n",
       "  '0.007*\"mom\" + 0.007*\"clinton\" + 0.005*\"husband\" + 0.005*\"cow\" + 0.004*\"ok\" + 0.004*\"wife\" + 0.003*\"parents\" + 0.003*\"pregnant\" + 0.003*\"movie\" + 0.003*\"ass\"')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"stupid\" + 0.004*\"dick\" + 0.004*\"religion\" + 0.004*\"jesus\" + 0.004*\"gun\" + 0.004*\"sense\" + 0.003*\"anybody\" + 0.003*\"em\" + 0.003*\"door\" + 0.003*\"christ\"'),\n",
       " (1,\n",
       "  '0.006*\"ass\" + 0.005*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.004*\"wife\" + 0.004*\"husband\" + 0.004*\"cow\" + 0.003*\"business\" + 0.003*\"movie\" + 0.003*\"ok\"'),\n",
       " (2,\n",
       "  '0.005*\"parents\" + 0.005*\"joke\" + 0.004*\"mom\" + 0.004*\"comedy\" + 0.004*\"bo\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.003*\"repeat\" + 0.003*\"love\" + 0.003*\"anthony\"'),\n",
       " (3,\n",
       "  '0.006*\"joke\" + 0.006*\"ahah\" + 0.004*\"gay\" + 0.004*\"nigga\" + 0.004*\"nuts\" + 0.004*\"rape\" + 0.004*\"jenner\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"twitter\"')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: mom, parents\n",
    "* Topic 1: husband, wife\n",
    "* Topic 2: guns\n",
    "* Topic 3: profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'ali'),\n",
       " (2, 'anthony'),\n",
       " (0, 'bill'),\n",
       " (2, 'bo'),\n",
       " (3, 'dave'),\n",
       " (2, 'hasan'),\n",
       " (1, 'jim'),\n",
       " (0, 'joe'),\n",
       " (1, 'john'),\n",
       " (2, 'louis'),\n",
       " (2, 'mike'),\n",
       " (3, 'ricky')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For a first pass of LDA, these kind of make sense to me, so we'll call it a day for now.\n",
    "* Topic 0: mom, parents [Anthony, Hasan, Louis, Ricky]\n",
    "* Topic 1: husband, wife [Ali, John, Mike]\n",
    "* Topic 2: guns [Bill, Bo, Jim]\n",
    "* Topic 3: profanity [Dave, Joe]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Additional Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try further modifying the parameters of the topic models above and see if you can get better topics.\n",
    "2. Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and see if you can get better topics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANS 1. We can try modifying various parameters like num_topics , passes , update_every , chunksize , id2word , iterations , dtype , minimum_probability and many more..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### changing num_topics=7,passes=50,minimum_probability=0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"tit\" + 0.006*\"ok\" + 0.005*\"ha\" + 0.005*\"murder\" + 0.005*\"date\" + 0.005*\"older\" + 0.005*\"worst\" + 0.005*\"food\" + 0.004*\"tits\" + 0.004*\"dog\"'),\n",
       " (1,\n",
       "  '0.004*\"joke\" + 0.003*\"friend\" + 0.003*\"bo\" + 0.003*\"jokes\" + 0.003*\"guns\" + 0.003*\"um\" + 0.003*\"jenny\" + 0.003*\"comedy\" + 0.003*\"repeat\" + 0.003*\"mad\"'),\n",
       " (2,\n",
       "  '0.009*\"mom\" + 0.006*\"parents\" + 0.005*\"hasan\" + 0.005*\"clinton\" + 0.003*\"husband\" + 0.003*\"cow\" + 0.003*\"york\" + 0.003*\"brown\" + 0.003*\"wife\" + 0.003*\"ok\"'),\n",
       " (3,\n",
       "  '0.007*\"joke\" + 0.004*\"nuts\" + 0.004*\"hell\" + 0.004*\"gun\" + 0.003*\"jenner\" + 0.003*\"dead\" + 0.003*\"rape\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"jesus\"')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### changing num_topics=7,passes=50,minimum_probability=0.002\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80,minimum_probability=0.002)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### changing num_topics=5,passes=70,minimum_probability=0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"joke\" + 0.005*\"mom\" + 0.005*\"jokes\" + 0.004*\"bo\" + 0.004*\"clinton\" + 0.003*\"repeat\" + 0.003*\"parents\" + 0.003*\"eye\" + 0.003*\"anthony\" + 0.003*\"stupid\"'),\n",
       " (1,\n",
       "  '0.010*\"hasan\" + 0.008*\"mom\" + 0.007*\"parents\" + 0.006*\"brown\" + 0.005*\"bike\" + 0.005*\"birthday\" + 0.004*\"york\" + 0.004*\"door\" + 0.003*\"pizza\" + 0.003*\"bethany\"'),\n",
       " (2,\n",
       "  '0.009*\"husband\" + 0.008*\"ok\" + 0.006*\"pregnant\" + 0.005*\"asian\" + 0.004*\"fingers\" + 0.004*\"doo\" + 0.004*\"toilet\" + 0.004*\"wan\" + 0.004*\"mom\" + 0.004*\"body\"'),\n",
       " (3,\n",
       "  '0.000*\"joke\" + 0.000*\"mom\" + 0.000*\"jenner\" + 0.000*\"jokes\" + 0.000*\"parents\" + 0.000*\"nuts\" + 0.000*\"hell\" + 0.000*\"friend\" + 0.000*\"doctor\" + 0.000*\"door\"'),\n",
       " (4,\n",
       "  '0.004*\"gun\" + 0.004*\"son\" + 0.003*\"jenny\" + 0.003*\"guns\" + 0.003*\"dog\" + 0.003*\"ahah\" + 0.003*\"ass\" + 0.003*\"class\" + 0.003*\"gay\" + 0.003*\"morning\"')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### changing num_topics=5,passes=70,minimum_probability=0.002\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=70,minimum_probability=0.002)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics:\n",
    "1.Joke\n",
    "2.Mom\n",
    "3.Husband\n",
    "4.Gun"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### changing num_topics=10,passes=150,minimum_probability=0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"joke\" + 0.007*\"jokes\" + 0.007*\"anthony\" + 0.006*\"twitter\" + 0.005*\"nuts\" + 0.005*\"grandma\" + 0.005*\"jenner\" + 0.005*\"dead\" + 0.004*\"chimp\" + 0.004*\"hampstead\"'),\n",
       " (1,\n",
       "  '0.006*\"clinton\" + 0.006*\"wife\" + 0.006*\"ahah\" + 0.005*\"mom\" + 0.005*\"gay\" + 0.005*\"tit\" + 0.004*\"cow\" + 0.004*\"young\" + 0.004*\"friend\" + 0.004*\"parents\"'),\n",
       " (2,\n",
       "  '0.010*\"bo\" + 0.008*\"repeat\" + 0.007*\"eye\" + 0.007*\"contact\" + 0.005*\"cos\" + 0.005*\"um\" + 0.005*\"brain\" + 0.005*\"stupid\" + 0.004*\"sad\" + 0.004*\"dick\"'),\n",
       " (3,\n",
       "  '0.000*\"pigeon\" + 0.000*\"pier\" + 0.000*\"pilots\" + 0.000*\"pilot\" + 0.000*\"pills\" + 0.000*\"pill\" + 0.000*\"pigs\" + 0.000*\"piggy\" + 0.000*\"pigeontoed\" + 0.000*\"pink\"'),\n",
       " (4,\n",
       "  '0.012*\"guns\" + 0.009*\"ass\" + 0.008*\"girlfriend\" + 0.007*\"gun\" + 0.007*\"class\" + 0.007*\"cunt\" + 0.006*\"business\" + 0.006*\"son\" + 0.006*\"south\" + 0.006*\"girls\"'),\n",
       " (5,\n",
       "  '0.000*\"pigeon\" + 0.000*\"pier\" + 0.000*\"pilots\" + 0.000*\"pilot\" + 0.000*\"pills\" + 0.000*\"pill\" + 0.000*\"pigs\" + 0.000*\"piggy\" + 0.000*\"pigeontoed\" + 0.000*\"pink\"'),\n",
       " (6,\n",
       "  '0.009*\"mom\" + 0.008*\"hasan\" + 0.006*\"husband\" + 0.005*\"parents\" + 0.005*\"brown\" + 0.005*\"ok\" + 0.004*\"bike\" + 0.004*\"pregnant\" + 0.004*\"birthday\" + 0.003*\"york\"'),\n",
       " (7,\n",
       "  '0.000*\"pigeon\" + 0.000*\"pier\" + 0.000*\"pilots\" + 0.000*\"pilot\" + 0.000*\"pills\" + 0.000*\"pill\" + 0.000*\"pigs\" + 0.000*\"piggy\" + 0.000*\"pigeontoed\" + 0.000*\"pink\"'),\n",
       " (8,\n",
       "  '0.007*\"jenny\" + 0.004*\"gun\" + 0.004*\"morning\" + 0.004*\"sudden\" + 0.004*\"accident\" + 0.004*\"sense\" + 0.004*\"religion\" + 0.003*\"parents\" + 0.003*\"sort\" + 0.003*\"idea\"'),\n",
       " (9,\n",
       "  '0.000*\"pigeon\" + 0.000*\"pier\" + 0.000*\"pilots\" + 0.000*\"pilot\" + 0.000*\"pills\" + 0.000*\"pill\" + 0.000*\"pigs\" + 0.000*\"piggy\" + 0.000*\"pigeontoed\" + 0.000*\"pink\"')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### changing num_topics=10,passes=150,minimum_probability=0.002\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=10, id2word=id2wordna, passes=150,minimum_probability=0.002)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans 2. We can try using different parts of speech like conjunction , prepositon , adverbs , plural nouns etc ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using plural noun & adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def pluralnouns_adverb(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun_adverb = lambda pos: pos[:2] =='NNS'or pos[:2] =='RB'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun_adverb(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>so very very yes because now dont even just th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>so much so here most politically not not im no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>very much here here here here ridiculously the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>here so touch alone as well now contractially ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>very much here very im not ever detroit well v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>back here very recently very very then very no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>very sweet right now very now so very really n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>im na not not right not also not not yet just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>there pretty again very now then so too too fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>very necessarily very much well easily around ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>here so ago pretty really sometimes so distinc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>just not just well actually so not obviously t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      so very very yes because now dont even just th...\n",
       "anthony  so much so here most politically not not im no...\n",
       "bill     very much here here here here ridiculously the...\n",
       "bo       here so touch alone as well now contractially ...\n",
       "dave     very much here very im not ever detroit well v...\n",
       "hasan    back here very recently very very then very no...\n",
       "jim      very sweet right now very now so very really n...\n",
       "joe      im na not not right not also not not yet just ...\n",
       "john     there pretty again very now then so too too fa...\n",
       "louis    very necessarily very much well easily around ...\n",
       "mike     here so ago pretty really sometimes so distinc...\n",
       "ricky    just not just well actually so not obviously t..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "import pandas as pd\n",
    "data_nouns_adv = pd.DataFrame(data_clean.transcript.apply(pluralnouns_adverb))\n",
    "data_nouns_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abroad</th>\n",
       "      <th>absofuckinglutely</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abstain</th>\n",
       "      <th>academically</th>\n",
       "      <th>accidentally</th>\n",
       "      <th>accordingly</th>\n",
       "      <th>admittedly</th>\n",
       "      <th>aggressively</th>\n",
       "      <th>ago</th>\n",
       "      <th>...</th>\n",
       "      <th>wrongly</th>\n",
       "      <th>ya</th>\n",
       "      <th>yeaaa</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yoga</th>\n",
       "      <th>yolo</th>\n",
       "      <th>youd</th>\n",
       "      <th>youll</th>\n",
       "      <th>youve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 368 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         abroad  absofuckinglutely  absolutely  abstain  academically  \\\n",
       "ali           0                  0           0        0             0   \n",
       "anthony       0                  0           0        0             0   \n",
       "bill          0                  0           3        0             0   \n",
       "bo            0                  0           3        2             0   \n",
       "dave          0                  0           0        0             0   \n",
       "hasan         0                  1           0        0             1   \n",
       "jim           3                  0           0        0             0   \n",
       "joe           0                  0           0        0             0   \n",
       "john          0                  0           1        0             0   \n",
       "louis         0                  0           0        0             0   \n",
       "mike          0                  0           0        0             0   \n",
       "ricky         0                  0           0        0             0   \n",
       "\n",
       "         accidentally  accordingly  admittedly  aggressively  ago  ...  \\\n",
       "ali                 1            0           0             1    1  ...   \n",
       "anthony             1            0           0             0    1  ...   \n",
       "bill                2            0           0             0    2  ...   \n",
       "bo                  1            0           0             0    0  ...   \n",
       "dave                0            0           0             0    2  ...   \n",
       "hasan               0            1           0             0    1  ...   \n",
       "jim                 0            0           0             0    0  ...   \n",
       "joe                 1            0           0             0    0  ...   \n",
       "john                0            0           0             0    2  ...   \n",
       "louis               0            0           0             0    2  ...   \n",
       "mike                0            0           0             0    1  ...   \n",
       "ricky               0            0           1             0    5  ...   \n",
       "\n",
       "         wrongly  ya  yeaaa  yep  yes  yoga  yolo  youd  youll  youve  \n",
       "ali            0   0      0    0    4     1     1     0      0      2  \n",
       "anthony        0   0      0    0    0     0     0     1      0      1  \n",
       "bill           0   1      0    0    0     0     0     1      0      0  \n",
       "bo             0   0      1    0    0     0     0     0      1      4  \n",
       "dave           0   0      0    0    2     0     0     1      1      0  \n",
       "hasan          0   0      0    0    1     0     0     2      0      2  \n",
       "jim            1   0      0    0    1     0     0     0      0      1  \n",
       "joe            0   1      0    0    0     0     0     1      0      2  \n",
       "john           0   0      0    0    3     0     0     1      0      0  \n",
       "louis          0   0      0    0    3     0     0     1      0      0  \n",
       "mike           0   0      0    0    1     0     0     2      0      0  \n",
       "ricky          0   0      0    1    6     0     0     0      0      1  \n",
       "\n",
       "[12 rows x 368 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adv.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names_out())\n",
    "data_dtmna.index = data_nouns_adv.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"basically\" + 0.020*\"finally\" + 0.018*\"anymore\" + 0.018*\"soon\" + 0.016*\"oh\" + 0.016*\"pretty\" + 0.012*\"absolutely\" + 0.012*\"ill\" + 0.012*\"twice\" + 0.012*\"obviously\"'),\n",
       " (1,\n",
       "  '0.027*\"ta\" + 0.025*\"anymore\" + 0.022*\"yes\" + 0.021*\"pretty\" + 0.018*\"totally\" + 0.016*\"definitely\" + 0.016*\"ago\" + 0.015*\"exactly\" + 0.012*\"eventually\" + 0.012*\"soon\"')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.035*\"anymore\" + 0.027*\"yes\" + 0.027*\"ta\" + 0.019*\"ago\" + 0.019*\"obviously\" + 0.016*\"immediately\" + 0.014*\"id\" + 0.014*\"especially\" + 0.014*\"honestly\" + 0.014*\"finally\"'),\n",
       " (1,\n",
       "  '0.032*\"pretty\" + 0.030*\"anymore\" + 0.023*\"basically\" + 0.021*\"totally\" + 0.019*\"oh\" + 0.017*\"soon\" + 0.017*\"finally\" + 0.015*\"yes\" + 0.014*\"ill\" + 0.014*\"ago\"'),\n",
       " (2,\n",
       "  '0.038*\"ta\" + 0.025*\"exactly\" + 0.022*\"definitely\" + 0.022*\"pretty\" + 0.019*\"youve\" + 0.016*\"recently\" + 0.016*\"suddenly\" + 0.013*\"billy\" + 0.013*\"totally\" + 0.013*\"finally\"'),\n",
       " (3,\n",
       "  '0.021*\"late\" + 0.015*\"barely\" + 0.015*\"youd\" + 0.015*\"forever\" + 0.015*\"youve\" + 0.015*\"soon\" + 0.015*\"aisha\" + 0.015*\"didnt\" + 0.008*\"physically\" + 0.008*\"longer\"')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now with 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=50)\n",
    "ldana.print_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b72fecbc99b9e9c3fbcc802f07cbc906940a8fa7aaf830932c7123d0afe84c65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
